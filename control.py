import argparse
import json
import logging
import os
import sys

from scrapy import signals
from scrapy.crawler import CrawlerRunner
from scrapy.settings import Settings
from scrapy.utils import misc, spider
from scrapy.xlib.pydispatch import dispatcher
from scrapy.utils.log import configure_logging
from twisted import internet

from postscraper import mymailsender
from postscraper import autogenerate
from postscraper import utils
from postscraper import settings


configure_logging()
LOG = logging.getLogger(__name__)
# crawlers that are running
RUNNING_CRAWLERS = []
FINISHED_CRAWLERS = []


def send_mail():
    body = "\n".join([s.email for s in FINISHED_CRAWLERS if s.email])
    if body:
        mailer = mymailsender.MailSender.from_settings(
            Settings(settings.MAILER_SETTINGS))
        mailer.send(to=settings.MAIL_RECIPIENT_LIST,
                    subject="New items from scraper",
                    body=body,
                    mimetype="text/html; charset=utf-8")
    internet.reactor.stop()


def on_close(spider):
    """
    Activates on spider closed signal
    """
    # for debug purposes
    RUNNING_CRAWLERS.remove(type(spider))
    FINISHED_CRAWLERS.append(spider)
    LOG.info("Spider closed: %s" % spider)


def _find_spiders():
    """Find all classes that subclass scrapy.Spider"""

    spider_map = {}

    def _get_spiders(spiders, spider_map):
        """Returns a list of all spiders with unique name found in a module.

        If 2 spiders with the same name are found, that subclass one another,
        then the child one is taken (based on mro)
        """
        # if two spiders with the same name are found, then take the one that
        # subclasses the autogenerated
        for s in spiders:
            if s.name in spider_map:
                # leave only the one that subclasses parent
                old = spider_map[s.name]
                if old in s.mro():
                    spider_map[s.name] = s
            else:
                spider_map[s.name] = s
        # the same one as passed with new values
        return spider_map

    for module in misc.walk_modules(settings.NEWSPIDER_MODULE):
        # crawl responsibly
        spiders = [s for s in spider.iter_spider_classes(module)]
        _get_spiders(spiders, spider_map)
    # add user generated modules
    user_spiders = auto(settings.USER_SPIDERS_FILE)
    _get_spiders(user_spiders, spider_map)
    # check for name uniqueness
    return spider_map.values()


def crawl_all(token=None):
    if not token:
        LOG.warn("No token passed, "
                 "acquiring one using login data from settings")
        token = utils.get_access_token()
    LOG.info("Access token: %s" % token)
    runner = CrawlerRunner(settings=Settings(
        {'DOWNLOAD_DELAY': settings.DOWNLOAD_DELAY,
         'ITEM_PIPELINES': settings.ITEM_PIPELINES}))

    dispatcher.connect(on_close, signal=signals.spider_closed)
    for spider_cls in _find_spiders():
        # FIXME incapsulation vialation
        # inject access_token to a VK spider
        spider_cls.access_token = token
        RUNNING_CRAWLERS.append(spider_cls)
        runner.crawl(spider_cls)
    d = runner.join()
    d.addBoth(lambda _: send_mail())

    internet.reactor.run()


def export(stream=sys.stdout):
    """Export file format: json_repr [SEPARATOR json_repr] """
    json_list = [s.to_dict() for s in _find_spiders() if s.type == "vk"]
    for s_repr in json_list:
        json.dump(s_repr, stream, separators=(',', ': '), indent=2)
        stream.write(settings.SPIDER_SEPARATOR)


def import_spiders(filename):
    with open(filename) as f:
        data = f.read().strip().split(settings.SPIDER_SEPARATOR)
        spiders = [json.loads(s) for s in data]
        autogenerate.create_spider_module(settings.AUTOGENERATED_SPIDERS_FILE,
                                          spiders)


def auto(filename, write=False):
    # convert data to json first
    """Load spiders from 'filename'.json.

    Return a list of autogenerated spiders.
    Physical modules will be created with 'write'=True.
    """
    if not os.path.exists(filename):
        LOG.warn("No file with user spiders '%s' found" % filename)
        return []
    with open(filename) as f:
        data = f.read().strip().split(settings.SPIDER_SEPARATOR)
        spiders_data = [json.loads(s) for s in data]
    if write:
        raise NotImplemented("Write func not implemented yet")
    spiders = [autogenerate.gen_spider(
        sd, module=settings.NEWSPIDER_MODULE + '.%s' % sd['name'])
        for sd in spiders_data]
    return spiders


def main():
    commands_map = {'crawl_all': (crawl_all, 'token'), 'export': (export, ),
                    'import': (import_spiders, 'file'),
                    'auto': (auto, 'json')}
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(dest='command')
    crawlall_parser = subparsers.add_parser(
        'crawl_all', help='Run all registered spiders')

    crawlall_parser.add_argument('--token',
                                 help='Access token for private VK info')
    import_parser = subparsers.add_parser(
        'import', help='Import spiders from a json file')
    import_parser.add_argument('file',
                               help='A file with spider data in json format')
    subparsers.add_parser('export',
                          help="Export all registered spiders' data as json")
    auto_parser = subparsers.add_parser('auto',
                                        help='Auto spider generation')
    auto_parser.add_argument('json', help='A json file with spider_data')
    args = parser.parse_args()
    if args.command:
        func_data = commands_map[args.command]
        if len(func_data) == 1:
            func_data[0]()
        else:
            func_data[0](*[getattr(args, v) for v in func_data[1:]])


if __name__ == "__main__":
    main()
