import argparse
import json
import logging
import sys

from scrapy import signals
from scrapy.crawler import CrawlerRunner
from scrapy.settings import Settings
from scrapy.xlib.pydispatch import dispatcher
from scrapy.utils.log import configure_logging
from twisted import internet

from postscraper import mymailsender
from postscraper import autogenerate
from postscraper import spider_utils
from postscraper import settings
from postscraper import utils


configure_logging()
LOG = logging.getLogger(__name__)
# crawlers that are running
RUNNING_CRAWLERS = []
FINISHED_CRAWLERS = []


def send_mail():
    body = "\n".join([s.email for s in FINISHED_CRAWLERS if s.email])
    if body:
        mailer = mymailsender.MailSender.from_settings(
            Settings(settings.MAILER_SETTINGS))
        mailer.send(to=settings.MAIL_RECIPIENT_LIST,
                    subject="New items from scraper",
                    body=body,
                    mimetype="text/html; charset=utf-8")
    internet.reactor.stop()


def on_close(spider):
    """
    Activates on spider closed signal
    """
    # for debug purposes
    RUNNING_CRAWLERS.remove(type(spider))
    FINISHED_CRAWLERS.append(spider)
    LOG.info("Spider closed: %s" % spider)


def crawl_all(token=None):
    if not token:
        LOG.warn("No token passed, "
                 "acquiring one using login data from settings")
        token = utils.get_access_token()
    LOG.info("Access token: %s" % token)
    runner = CrawlerRunner(settings=Settings(
        {'DOWNLOAD_DELAY': settings.DOWNLOAD_DELAY,
         'ITEM_PIPELINES': settings.ITEM_PIPELINES}))

    dispatcher.connect(on_close, signal=signals.spider_closed)
    for spider_cls in spider_utils.find_spiders():
        # FIXME incapsulation vialation
        # inject access_token to a VK spider
        spider_cls.access_token = token
        RUNNING_CRAWLERS.append(spider_cls)
        runner.crawl(spider_cls)
    d = runner.join()
    d.addBoth(lambda _: send_mail())

    internet.reactor.run()


def export(stream=sys.stdout):
    """Export file format: json_repr [SEPARATOR json_repr] """
    json_list = [s.to_dict() for s in spider_utils.find_spiders()
                 if s.type == "vk"]
    for s_repr in json_list:
        json.dump(s_repr, stream, separators=(',', ': '), indent=2)
        stream.write(settings.SPIDER_SEPARATOR)


def import_spiders(filename):
    with open(filename) as f:
        data = f.read().strip().split(settings.SPIDER_SEPARATOR)
        spiders = [json.loads(s) for s in data]
        autogenerate.create_spider_module(settings.AUTOGENERATED_SPIDERS_FILE,
                                          spiders)


def main():
    commands_map = {'crawl_all': (crawl_all, 'token'), 'export': (export, ),
                    'import': (import_spiders, 'file'),
                    'auto': (autogenerate.load_spiders_from_json, 'json')}
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(dest='command')
    crawlall_parser = subparsers.add_parser(
        'crawl_all', help='Run all registered spiders')

    crawlall_parser.add_argument('--token',
                                 help='Access token for private VK info')
    import_parser = subparsers.add_parser(
        'import', help='Import spiders from a json file')
    import_parser.add_argument('file',
                               help='A file with spider data in json format')
    subparsers.add_parser('export',
                          help="Export all registered spiders' data as json")
    auto_parser = subparsers.add_parser('auto',
                                        help='Auto spider generation')
    auto_parser.add_argument('json', help='A json file with spider_data')
    args = parser.parse_args()
    if args.command:
        func_data = commands_map[args.command]
        if len(func_data) == 1:
            func_data[0]()
        else:
            func_data[0](*[getattr(args, v) for v in func_data[1:]])


if __name__ == "__main__":
    main()
